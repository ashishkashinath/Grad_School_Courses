<!DOCTYPE html>
<!-- saved from url=(0046)https://plato.stanford.edu/entries/it-privacy/ -->
<html><!-- <![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>
Privacy and Information Technology (Stanford Encyclopedia of Philosophy)
</title>

<meta name="robots" content="noarchive, noodp">
<meta name="citation_title" content="Privacy and Information Technology">
<meta name="citation_author" content="van den Hoven, Jeroen">
<meta name="citation_author" content="Blaauw, Martijn">
<meta name="citation_author" content="Pieters, Wolter">
<meta name="citation_author" content="Warnier, Martijn">
<meta name="citation_publication_date" content="2014/11/20">
<meta name="DC.title" content="Privacy and Information Technology">
<meta name="DC.creator" content="van den Hoven, Jeroen">
<meta name="DC.creator" content="Blaauw, Martijn">
<meta name="DC.creator" content="Pieters, Wolter">
<meta name="DC.creator" content="Warnier, Martijn">
<meta name="DCTERMS.issued" scheme="DCTERMS.W3CDTF" content="2014-11-20">
<meta name="DCTERMS.modified" scheme="DCTERMS.W3CDTF" content="2014-11-20">

<!-- NOTE: Import webfonts using this link: -->
<link href="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/css" rel="stylesheet" type="text/css">

<link rel="stylesheet" type="text/css" media="screen,handheld" href="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/bootstrap.min.css">
<link rel="stylesheet" type="text/css" media="screen,handheld" href="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/bootstrap-responsive.min.css">
<link rel="stylesheet" type="text/css" href="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/font-awesome.min.css">
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen,handheld" href="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/style.css">
<link rel="stylesheet" type="text/css" media="print" href="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/print.css">
<link rel="stylesheet" type="text/css" href="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/entry.css">
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script async="" src="https://www.google-analytics.com/analytics.js"></script><script type="text/javascript" src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/jquery-1.9.1.min.js.download"></script><style></style>
<script type="text/javascript" src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/bootstrap.min.js.download"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/jquery-scrolltofixed-min.js.download"></script>
<script type="text/javascript" src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/entry.js.download"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/sep.js.download"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="article" id="pagetopright"><div id="StayFocusd-infobar" style="display: none; top: 0px;">
    <img src="chrome-extension://laankejkbhbdhmipfmgcngdelahlfoji/common/img/eye_19x19_red.png">
    <span id="StayFocusd-infobar-msg"></span>
    <span id="StayFocusd-infobar-links">
        <a id="StayFocusd-infobar-never-show">hide forever</a>&nbsp;&nbsp;|&nbsp;&nbsp;
        <a id="StayFocusd-infobar-hide">hide once</a>
    </span>
</div>
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="https://plato.stanford.edu/index.html"><img src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/sep-man-red.png" alt="SEP logo"></a></div>
      <div id="site-title"><a href="https://plato.stanford.edu/index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu collapse">
              <ul class="nav">
                <li class="dropdown"><a id="drop1" href="https://plato.stanford.edu/entries/it-privacy/#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li><a href="https://plato.stanford.edu/contents.html">Table of Contents</a></li>
                    <li><a href="https://plato.stanford.edu/new.html">What's New</a></li>
                    <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li><a href="https://plato.stanford.edu/published.html">Chronological</a></li>
                    <li><a href="https://plato.stanford.edu/archives/">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown"><a id="drop2" href="https://plato.stanford.edu/entries/it-privacy/#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li><a href="https://plato.stanford.edu/info.html">Editorial Information</a></li>
                    <li><a href="https://plato.stanford.edu/about.html">About the SEP</a></li>
                    <li><a href="https://plato.stanford.edu/board.html">Editorial Board</a></li>
                    <li><a href="https://plato.stanford.edu/cite.html">How to Cite the SEP</a></li>
                    <li><a href="https://plato.stanford.edu/special-characters.html">Special Characters</a></li>
                    <li><a href="https://plato.stanford.edu/tools/">Advanced Tools</a></li>
                    <li><a href="https://plato.stanford.edu/contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown"><a id="drop3" href="https://plato.stanford.edu/entries/it-privacy/#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li><a href="https://plato.stanford.edu/support/">Support the SEP</a></li>
                    <li><a href="https://plato.stanford.edu/support/friends.html">PDFs for SEP Friends</a></li>
                    <li><a href="https://plato.stanford.edu/support/donate.html">Make a Donation</a></li>
                    <li><a href="https://plato.stanford.edu/support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="https://plato.stanford.edu/search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP">
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky" style="z-index: 999; position: static; top: 482.167px;">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar collapsed" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar collapse">
          <ul class="nav">
            <li><a href="https://plato.stanford.edu/entries/it-privacy/#toc">Entry Contents</a></li>
            <li><a href="https://plato.stanford.edu/entries/it-privacy/#Bib">Bibliography</a></li>
            <li><a href="https://plato.stanford.edu/entries/it-privacy/#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/it-privacy/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=it-privacy">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="https://plato.stanford.edu/entries/it-privacy/#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div><div style="display: none; width: 242px; height: 244px; float: left;"></div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Privacy and Information Technology</h1><div id="pubinfo"><em>First published Thu Nov 20, 2014</em></div>

<div id="preamble">

<p>Human beings value their privacy and the protection of their
personal sphere of life. They value some control over who knows what
about them. They certainly do not want their personal information to
be accessible to just anyone at any time. But recent advances in
information technology threaten privacy and have reduced the amount of
control over personal data and open up the possibility of a range of
negative consequences as a result of access to personal data. The
21<sup>st</sup> century has become the century of Big Data and
advanced Information Technology allows for the storage and processing
of exabytes of data. The revelations of Edward Snowden have
demonstrated that these worries are real and that the technical
capabilities to collect, store and search large quantities of data
concerning telephone conversations, internet searches and electronic
payment are now in place and are routinely used by government
agencies. For business firms, personal data about customers and
potential customers are now also a key asset. At the same time, the
meaning and value of privacy remains the subject of considerable
controversy. The combination of increasing power of new technology and
the declining clarity and agreement on privacy give rise to problems
concerning law, policy and ethics. The focus of this article is on
exploring the relationship between information technology (IT) and
privacy. We will both illustrate the specific threats that IT and
innovations in IT pose for privacy, and indicate how
IT <em>itself</em> might be able to overcome these privacy concerns by
being developed in a ‘privacy-sensitive way’. We will also
discuss the role of emerging technologies in the debate, and account
for the way in which moral debates are themselves affected by IT.</p>

</div>

<div id="toc">
<!--Entry Contents-->
<ul>
<li><a href="https://plato.stanford.edu/entries/it-privacy/#ConPriValPri">1. Conceptions of privacy and the value of privacy</a>
  <ul>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#ConVsInfPri">1.1 Constitutional vs. informational privacy</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#AccValPri">1.2 Accounts of the value of privacy</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#PerDat">1.3 Personal Data</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#MorReaForProPerDat">1.4 Moral reasons for protecting personal data</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#LawRegIndConOveAcc">1.5 Law, regulation, and indirect control over access</a></li>
  </ul></li>
<li><a href="https://plato.stanford.edu/entries/it-privacy/#ImpInfTecPri">2. The impact of information technology on privacy </a>
  <ul>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#DevInfTec">2.1 Developments in information technology</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#Int">2.2 Internet</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#SocMed">2.3 Social media</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#BigDat">2.4 Big Data</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#MobDev">2.5 Mobile devices</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#IntThi">2.6 The Internet of Things</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#EGov">2.7 E-Government </a></li>
  </ul></li>
<li><a href="https://plato.stanford.edu/entries/it-privacy/#HowInfTecItsSolPriCon">3. How can information technology itself solve privacy concerns?</a>
  <ul>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#DesMet">3.1 Design methods</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#PriEnhTec">3.2 Privacy enhancing technologies</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#Cry">3.3 Cryptography</a></li>
  <li><a href="https://plato.stanford.edu/entries/it-privacy/#IdeMan">3.4 Identity management</a></li>
  </ul></li>
<li><a href="https://plato.stanford.edu/entries/it-privacy/#EmeTecOurUndPri">4. Emerging technologies and our understanding of privacy</a></li>
<li><a href="https://plato.stanford.edu/entries/it-privacy/#Bib">Bibliography</a></li>
<li><a href="https://plato.stanford.edu/entries/it-privacy/#Aca">Academic Tools</a></li>
<li><a href="https://plato.stanford.edu/entries/it-privacy/#Oth">Other Internet Resources</a></li>
<li><a href="https://plato.stanford.edu/entries/it-privacy/#Rel">Related Entries</a></li>
</ul>

<!--Entry Contents-->
<hr>
</div>

<div id="main-text">

<h2><a id="ConPriValPri">1. Conceptions of privacy and the value of privacy</a></h2>

<p>Discussions about privacy are intertwined with the use of
technology. The publication that began the debate about privacy in the
Western world was occasioned by the introduction of the newspaper
printing press and photography. Samuel D. Warren and Louis Brandeis
wrote their article on privacy in the Harvard Law Review (Warren &amp;
Brandeis 1890) partly in protest against the intrusive activities of
the journalists of those days. They argued that there is a
“right to be left alone” based on a principle of
“inviolate personality”. Since the publication of that
article, the debate about privacy has been fueled by claims for the
right of individuals to determine the extent to which others have
access to them (Westin 1967) and claims for the right of society to
know about individuals. The privacy debate has co-evolved with the
development of information technology. It is therefore difficult to
conceive of the notions of privacy and discussions about data
protection as separate from the way computers, the Internet, mobile
computing and the many applications of these basic technologies have
evolved.</p>

<h3><a id="ConVsInfPri">1.1 Constitutional vs. informational privacy</a></h3>

<p>Inspired by subsequent developments in U.S. law, a distinction can
be made between (1) <em>constitutional</em> (or
decisional) <em>privacy</em> and (2) <em>tort</em> (or
informational) <em>privacy</em> (DeCew 1997). The first refers to the
freedom to make one's own decisions without interference by others in
regard to matters seen as intimate and personal, such as the decision
to use contraceptives or to have an abortion. The second is concerned
with the interest of individuals in exercising control over access to
information about themselves and is most often referred to as
“informational privacy”. Think here, for instance, about
information disclosed on Facebook or other social media. All too
easily, such information might be beyond the control of the
individual.</p>

<p>Statements about privacy can be either descriptive or normative,
depending on whether they are used to describe the way people define
situations and conditions of privacy and the way they value them, or
are used to indicate that there ought to be constraints on the use of
information or information processing. Informational privacy in a
normative sense refers typically to a non-absolute moral right of
persons to have direct or indirect control over access to (1)
information about oneself, (2) situations in which others could
acquire information about oneself, and (3) technology that can be used
to generate, process or disseminate information about oneself.</p>

<h3><a id="AccValPri">1.2 Accounts of the value of privacy</a></h3>

<p>The debates about privacy are almost always revolving around new
technology, ranging from genetics and the extensive study of
bio-markers, brain imaging, drones, wearable sensors and sensor
networks, social media, smart phones, closed circuit television, to
government cybersecurity programs, direct marketing, RFID tags, Big
Data, head-mounted displays and search engines. There are basically
two reactions to the flood of new technology and its impact on
personal information and privacy: the first reaction, held by many
people in IT industry and in R&amp;D, is that we have zero privacy in
the digital age and that there is no way we can protect it, so we
should get used to the new world and get over it. The other reaction
is that our privacy is more important than ever and that we can and we
must attempt to protect it.</p>

<p>In the literature on privacy, there are many competing accounts of
the nature and value of privacy. On one end of the
spectrum, <em>reductionist</em> accounts argue that privacy claims are
really about other values and other things that matter from a moral
point of view. According to these views the value of privacy is
reducible to these other values or sources of value (Thomson
1975). Proposals that have been defended along these lines mention
property rights, security, autonomy, intimacy or friendship,
democracy, liberty, dignity, or utility and economic
value. Reductionist accounts hold that the importance of privacy
should be explained and its meaning clarified in terms of those other
values and sources of value (Westin 1967). The opposing view holds
that privacy is valuable in itself and its value and importance are
not derived from other considerations (see for a discussion Rössler
2004). Views that construe privacy and the personal sphere of life as
a human right would be an example of this non-reductionist
conception.</p>

<p>More recently a type of privacy account has been proposed in
relation to new information technology, that acknowledges that there
is a cluster of related moral claims (cluster accounts) underlying
appeals to privacy (DeCew 1997; Solove 2006; van den Hoven 1999;
Allen 2011; Nissenbaum 2004), but maintains that there is no single
essential core of privacy concerns. A recent final addition to the
body of privacy accounts are epistemic accounts, where the notion of
privacy is analyzed primarily in terms of knowledge or other epistemic
states.  Having privacy means that others don't know certain private
propositions; lacking privacy means that others do know certain
private propositions (Blaauw 2013). An important aspect of this
conception of having privacy is that it is seen as a relation (Rubel
2011; Matheson 2007; Blaauw 2013) with three argument places: a
subject (<i>S</i>), a set of propositions (<i>P</i>) and a set of
individuals (<i>I</i>). Here <i>S</i> is the subject who has (a
certain degree of) privacy. <i>P</i> is composed of those propositions
the subject wants to keep private (call the propositions in this set
‘personal propositions’), and <i>I</i> is composed of
those individuals with respect to whom <i>S</i> wants to keep the
personal propositions private.</p>

<p>Another distinction that is useful to make is the one between a
European and a US American approach. A bibliometric study suggests
that the two approaches are separate in the literature. The first
conceptualizes issues of informational privacy in terms of ‘data
protection’, the second in terms of ‘privacy’
(Heersmink et al. 2011). In discussing the relationship of privacy
matters with technology, the notion of data protection is most
helpful, since it leads to a relatively clear picture of what the
object of protection is and by which technical means the data can be
protected.  At the same time it invites answers to the question why
the data ought to be protected. Informational privacy is thus recast
in terms of the protection of personal data (van den Hoven 2008).</p>

<h3><a id="PerDat">1.3 Personal Data</a></h3>

<p>Personal information or data is information or data that is linked
or can be linked to individual persons. Examples include date of
birth, sexual preference, whereabouts, religion, but also the IP
address of your computer or metadata pertaining to these kinds of
information.  Personal data can be contrasted with data that is
considered sensitive, valuable or important for other reasons, such as
secret recipes, financial data, or military intelligence. Data that is
used to secure other information, such as passwords, are not
considered here. Although such security measures may contribute to
privacy, their protection is only instrumental to the protection of
other information, and the quality of such security measures is
therefore out of the scope of our considerations here.</p>

<p>A relevant distinction that has been made in philosophical
semantics is that between the referential and the attributive use of
descriptive labels of persons (van den Hoven 2008). Personal data is
defined in the law as data that can be linked with a natural
person. There are two ways in which this link can be made; a
referential mode and a non-referential mode. The law is primarily
concerned with the ‘referential use’ of descriptions, the
type of use that is made on the basis of a (possible) acquaintance
relationship of the speaker with the object of his
knowledge. “The murderer of Kennedy must be insane”,
uttered while pointing to him in court is an example of a
referentially used description. This can be contrasted with
descriptions that are used attributively as in “the murderer of
Kennedy must be insane, whoever he is”. In this case, the user
of the description is not—and may never be—acquainted with
the person he is talking about or wants to refer to. If the legal
definition of personal data is interpreted referentially, much of the
data about persons would be unprotected; that is the processing of
this data would not be constrained on moral grounds related to privacy
or personal sphere of life.</p>

<h3><a id="MorReaForProPerDat">1.4 Moral reasons for protecting personal data</a></h3>

<p>The following types of moral reasons for the protection of personal
data and for providing direct or indirect control over access to those
data by others can be distinguished (van den Hoven 2008):</p>
<ol>
<li>Prevention of harm: Unrestricted access by others to one's
passwords, characteristics, and whereabouts can be used to harm the
data subject in a variety of ways.</li>

<li>Informational inequality: Personal data have become commodities.
Individuals are usually not in a good position to negotiate contracts
about the use of their data and do not have the means to check whether
partners live up to the terms of the contract. Data protection laws,
regulation and governance aim at establishing fair conditions for
drafting contracts about personal data transmission and exchange and
providing data subjects with checks and balances, guarantees for
redress.</li>

<li>Informational injustice and discrimination: Personal information
provided in one sphere or context (for example, health care) may
change its meaning when used in another sphere or context (such as
commercial transactions) and may lead to discrimination and
disadvantages for the individual.</li>

<li>Encroachment on moral autonomy: Lack of privacy may expose
individuals to outside forces that influence their choices.</li>
</ol>

<p>These formulations all provide good moral reasons for limiting and
constraining access to personal data and providing individuals with
control over their data.</p>

<h3><a id="LawRegIndConOveAcc">1.5 Law, regulation, and indirect control over access</a></h3>

<p>Data protection laws are in force in almost all countries. The
basic moral principle underlying these laws is the requirement of
informed consent for processing by the data subject. Furthermore,
processing of personal information requires that its purpose be
specified, its use be limited, individuals be notified and allowed to
correct inaccuracies, and the holder of the data be accountable to
oversight authorities (OECD 1980). Because it is impossible to
guarantee compliance of all types of data processing in all these
areas and applications with these rules and laws in traditional ways,
so-called privacy-enhancing technologies and identity management
systems are expected to replace human oversight in many cases. The
challenge with respect to privacy in the twenty-first century is to
assure that technology is designed in such a way that it incorporates
privacy requirements in the software, architecture, infrastructure,
and work processes in a way that makes privacy violations unlikely to
occur.</p>

<h2><a id="ImpInfTecPri">2. The impact of information technology on privacy</a></h2> 

<h3><a id="DevInfTec">2.1 Developments in information technology</a></h3>

<p>“Information technology” refers to automated systems
for storing, processing, and distributing information. Typically, this
involves the use of computers and communication networks. The amount
of information that can be stored or processed in an information
system depends on the technology used. The capacity of the technology
has increased rapidly over the past decades, in accordance with
Moore's law. This holds for storage capacity, processing capacity, and
communication bandwidth. We are now capable of storing and processing
data on the exabyte level. For illustration, to store 100 exabytes of
data on 720 MB CD-ROM discs would require a stack of them that would
almost reach the moon.</p>

<p>These developments have fundamentally changed our practices of
information provisioning. Even within the academic research field,
current practices of writing, submitting, reviewing and publishing
texts such as this one would be unthinkable without information
technology support. At the same time, many parties collate information
about publications, authors, etc. This enables recommendations on
which papers researchers should read, but at the same time builds a
detailed profile of each individual researcher.</p>

<p>The rapid changes have increased the need for careful consideration
of the desirability of effects. Some even speak of a digital
revolution as a technological leap similar to the industrial
revolution, or a digital revolution as a revolution in understanding
human nature and the world, similar to the revolutions of Copernicus,
Darwin and Freud (Floridi 2008). In both the technical and the
epistemic sense, emphasis has been put on connectivity and
interaction. Physical space has become less important, information is
ubiquitous, and social relations have adapted as well.</p>

<p>As we have described privacy in terms of moral reasons for imposing
constraints on access to and/or use of personal information, the
increased connectivity imposed by information technology poses many
questions. In a descriptive sense, access has increased, which, in a
normative sense, requires consideration of the desirability of this
development, and evaluation of the potential for regulation by
technology, institutions, and/or law.</p>

<p>As connectivity increases access to information, it also increases
the possibility for agents to <em>act</em> based on the new sources of
information. When these sources contain personal information, risks of
harm, inequality, discrimination, and loss of autonomy easily emerge.
For example, your enemies may have less difficulty finding out where
you are, users may be tempted to give up privacy for perceived
benefits in online environments, and employers may use online
information to avoid hiring certain groups of people. Furthermore,
systems rather than users may decide which information is displayed,
thus confronting users only with news that matches their profiles.</p>

<p>Although the technology operates on a device level, information
technology consists of a complex system of socio-technical practices,
and its context of use forms the basis for discussing its role in
changing possibilities for accessing information, and thereby
impacting privacy. We will discuss some specific developments and
their impact in the following sections.</p>

<h3><a id="Int">2.2 Internet</a></h3>

<p>The Internet, originally conceived in the 1960s and developed in
the 1980s as a scientific network for exchanging information, was not
designed for the purpose of separating information flows (Michener
1999). The World Wide Web of today was not foreseen, and neither was
the possibility of misuse of the Internet. Social network sites
emerged for use within a community of people who knew each other in
real life—at first, mostly in academic settings—rather
than being developed for a worldwide community of users (Ellison
2007). It was assumed that sharing with close friends would not cause
any harm, and privacy and security only appeared on the agenda when
the network grew larger. This means that privacy concerns often had to
be dealt with as add-ons rather than by-design.</p>

<p>A major theme in the discussion of Internet privacy revolves around
the use of cookies (Palmer 2005). Cookies are small pieces of data
that web sites store on the user's computer, in order to enable
personalization of the site. However, some cookies can be used to
track the user across multiple web sites (tracking cookies), enabling
for example advertisements for a product the user has recently viewed
on a totally different site. Again, it is not always clear what the
generated information is used for. Laws requiring user consent for the
use of cookies are not always successful, as the user may simply click
away any requests for consent, merely finding them
annoying. Similarly, features of social network sites embedded in
other sites (e.g., “like”-button) may allow the social
network site to identify the sites visited by the user (Krishnamurthy
&amp; Wills 2009).</p>

<p>The recent development of cloud computing increases the many
privacy concerns (Ruiter &amp; Warnier 2011). Previously, whereas
information would be available from the web, user data and programs
would still be stored locally, preventing program vendors from having
access to the data and usage statistics. In cloud computing, both data
and programs are online (in the cloud), and it is not always clear
what the user-generated and system-generated data are used
for. Moreover, as data is located elsewhere in the world, it is not
even always obvious which law is applicable, and which authorities can
demand access to the data. Data gathered by online services and apps
such as search engines and games are of particular concern here. Which
data is used and communicated by applications (browsing history,
contact lists, etc.) is not always clear, and even when it is, the
only choice available to the user may be not to use the
application. In general, IT services have more and different privacy
issues than IT products (Pieters 2013).</p>

<p>Some special features of Internet privacy (social media and Big
Data) are discussed in the following sections.</p>

<h3><a id="SocMed">2.3 Social media</a></h3>

<p>The interactive web, known as Web 2.0, where users generate much of
the content themselves, poses additional challenges. The question is
not merely about the moral reasons for limiting access to information,
it is also about the moral reasons for limiting
the <em>invitations</em> to users to submit all kinds of personal
information. Social network sites invite the user to generate more
data, to increase the value of the site (“your profile is
…% complete”). Users are <em>tempted</em> to exchange
their personal data for the benefits of using services, and provide
both this data and their attention as payment for the services. In
addition, users may not even be aware of what information they are
tempted to provide, as in the abovementioned case of the
“like”-button on other sites. Merely limiting the access
to personal information does not do justice to the issues here, and
the more fundamental question lies in steering the users' behavior of
sharing.</p>

<p>One way of limiting the temptation of users to share is requiring
default privacy settings to be strict. Even then, this limits access
for other users (“friends of friends”), but it does not
limit access for the service provider. Also, such restrictions limit
the value and usability of the social network sites themselves, and
may reduce positive effects of such services. A particular example of
privacy-friendly defaults is the opt-in as opposed to the opt-out
approach. When the user has to take an explicit action to share data
or to subscribe to a service or mailing list, the resulting effects
may be more acceptable to the user. However, much still depends on how
the choice is framed (Bellman, Johnson, &amp; Lohse 2001).</p>

<h3><a id="BigDat">2.4 Big Data</a></h3>

<p>Users generate loads of data when online. This is not only data
explicitly entered by the user, but also numerous statistics on user
behavior: sites visited, links clicked, search terms entered. Data
mining can be employed to extract patterns from such data, which can
then be used to make decisions about the user. These may only affect
the online experience (advertisements shown), but, depending on which
parties have access to the information, they may also impact the user
in completely different contexts.</p>

<p>In particular, Big Data may be used in profiling the user
(Hildebrandt 2008), creating patterns of typical combinations of user
properties, which can then be used to predict interests and behavior.
An innocent application is “you may also like …”,
but, depending on the available data, more sensitive derivations may
be made, such as most probable religion or sexual preference. These
derivations could then in turn lead to inequality or discrimination.
When a user can be assigned to a particular group, even only
probabilistically, this may influence the actions taken by others. For
example, profiling could lead to refusal of insurance or a credit
card, in which case profit is the main reason for
discrimination. Profiling could also be used by organizations or
possible future governments that have discrimination of particular
groups on their political agenda, in order to find their targets and
deny them access to services, or worse.</p>

<p>Big Data does not only emerge from Internet
transactions. Similarly, data may be collected when shopping, when
being recorded by surveillance cameras in public or private spaces, or
when using smartcard-based public transport payment systems. All these
data could be used to profile citizens, and base decisions upon such
profiles. For example, shopping data could be used to send information
about healthy food habits to particular individuals, but again also
for decisions on insurance. According to EU data protection law,
permission is needed for processing personal data, and they can only
be processed for the purpose for which they were obtained. Specific
challenges, therefore, are (a) how to obtain permission when the user
does not explicitly engage in a transaction (as in case of
surveillance), and (b) how to prevent “function creep”,
i.e., data being used for different purposes after they are collected
(as may happen for example with DNA databases (Dahl &amp; Sætnan
2009).</p>

<p>One particular concern could emerge from genetics data (Tavani
2004). Like other data, genomics can be used to predict, and in
particular could predict risks of diseases. Apart from others having
access to detailed user profiles, a fundamental question here is
whether the individual should know what is known about her. In
general, users could be said to have a right to access any information
stored about them, but in this case, there may also be a right not to
know, in particular when knowledge of the data (e.g., risks of
diseases) would reduce the well-being—by causing fear, for
instance—without enabling treatment. With respect to previous
examples, one may not want to know the patterns in one's own shopping
behavior either.</p>

<h3><a id="MobDev">2.5 Mobile devices</a></h3>

<p>As users increasingly own networked devices like cellphones, mobile
devices collect and send more and more data. These devices typically
contain a range of data-generating sensors, including GPS (location),
movement sensors, and cameras, and may transmit the resulting data via
the Internet or other networks. One particular example concerns
location data. Many mobile devices have a GPS sensor that registers
the user's location, but even without a GPS sensor, approximate
locations can be derived, for example by monitoring the available
wireless networks. As location data links the online world to the
user's physical environment, with the potential of physical harm
(stalking, burglary during holidays, etc.), such data are often
considered particularly sensitive.</p>

<p>Many of these devices also contain cameras which, when applications
have access, can be used to take pictures. These can be considered
sensors as well, and the data they generate may be particularly
private. For sensors like cameras, it is assumed that the user is
aware when they are activated, and privacy depends on such
knowledge. For webcams, a light typically indicates whether the camera
is on, but this light may be manipulated by malicious software. In
general, “reconfigurable technology” (Dechesne, Warnier,
&amp; van den Hoven 2011) that handles personal data raises the
question of user knowledge of the configuration.</p>

<h3><a id="IntThi">2.6 The Internet of Things</a></h3>

<p>Devices connected to the Internet are not limited to user-owned
computing devices like smartphones. Many devices contain chips and/or
are connected in the so-called Internet of Things. RFID (radio
frequency identification) chips can be read from a limited distance,
such that you can hold them in front of a reader rather than inserting
them. EU and US passports have RFID chips with protected biometric
data, but information like the user's nationality may easily leak when
attempting to read such devices (see Richter, Mostowski &amp; Poll
2008, in Other Internet Resources). “Smart” RFIDs are also
embedded in public transport payment systems. “Dumb”
RFIDs, basically only containing a number, appear in many kinds of
products as a replacement of the barcode, and for use in
logistics. Still, such chips could be used to trace a person once it
is known that he carries an item containing a chip.</p>

<p>In the home, there are smart meters for automatically reading and
sending electricity consumption, and thermostats and other devices
that can be remotely controlled by the owner. Such devices again
generate statistics, and these can be used for mining and
profiling. In the future, more and more household appliances will be
connected, each generating its own information. Ambient intelligence
(Brey 2005), and ubiquitous computing, along with the Internet of
Things (Friedewald &amp; Raabe 2011), also enable automatic adaptation
of the environment to the user, based on explicit preferences and
implicit observations, and user autonomy is a central theme in
considering the privacy implications of such devices.</p>

<h3><a id="EGov">2.7 E-Government</a></h3> 

<p>Government and public administration have undergone radical
transformations as a result of the availability of advanced IT systems
as well. Examples of these changes are biometric passports, online
e-government services, voting systems, a variety of online citizen
participation tools and platforms or online access to recordings of
sessions of parliament and government committee meetings. </p>

<p>Consider the case of voting in elections. Information technology
may play a role in different phases in the voting process, which may
have different impact on voter privacy. Most countries have a
requirement that elections are to be held by secret ballot, to prevent
vote buying and coercion. In this case, the voter is supposed to keep
her vote private, <em>even if she would want to reveal it</em>. For
information technology used for casting votes, this is defined as the
requirement of receipt-freeness or coercion-resistance (Delaune,
Kremer &amp; Ryan 2006). In polling stations, the authorities see to
it that the voter keeps the vote private, but such surveillance is not
possible when voting by mail or online, and it cannot even be enforced
by technological means, as someone can always watch while the voter
votes.  In this case, privacy is not only a right but also a duty, and
information technology developments play an important role in the
possibilities of the voter to fulfill this duty, as well as the
possibilities of the authorities to verify this. In a broader sense,
e-democracy initiatives may change the way privacy is viewed in the
political process.</p>

<h2><a id="HowInfTecItsSolPriCon">3. How can information technology itself solve privacy concerns?</a></h2>

<p>Whereas information technology is typically seen as
the <em>cause</em> of privacy problems, there are also several ways in
which information technology can help to solve these problems. There
are rules, guidelines or best practices that can be used for designing
privacy-preserving systems. Such possibilities range from
ethically-informed design methodologies to using encryption to protect
personal information from unauthorized use.</p>

<h3><a id="DesMet">3.1 Design methods</a></h3>

<p>Value Sensitive Design provides a “theoretically grounded
approach to the design of technology that accounts for human values in
a principled and comprehensive manner throughout the design
process” (Friedman et al. 2006).  It provides a set of rules and
guidelines for designing a system with a certain value in mind. One
such value can be ‘privacy’, and value sensitive design
can thus be used as a method to design privacy-friendly IT
systems. The ‘Privacy by Design’ approach as advocated by
Cavoukian (2009) and others can be regarded as one of the value
sensitive design approaches that specifically focuses on privacy. The
Privacy by Design approach provides high-level guidelines in the form
of seven principles for designing privacy-preserving systems. These
principles have at their core that “data protection needs to be
viewed in proactive rather then reactive terms, making privacy by
design preventive and not simply remedial” (Cavoukian
2010). Privacy by design's main point is that data protection should
be central in all phases of product life cycles, from initial design
to operational use and disposal. The Privacy Impact Assessment
approach proposed by Clarke (2009) makes a similar point. It proposes
“a systematic process for evaluating the potential effects on
privacy of a project, initiative or proposed system or scheme”
(Clarke 2009). Note that these approaches should not only be seen as
auditing approaches, but rather as a means to make privacy awareness
and compliance an integral part of the organizational and engineering
culture.</p>

<p>There are also several industry guidelines that can be used to
design privacy preserving IT systems. The Payment Card Industry Data
Security Standard (see PCI DSS v3.0, 2013, in the Other Internet
Resources), for example, gives very clear guidelines for privacy and
security sensitive systems design in the domain of the credit card
industry and its partners (retailers, banks). Various International
Organization for Standardization (ISO) standards (Hone &amp; Eloff
2002) also serve as a source of best practices and guidelines,
especially with respect to security, for the design of privacy
friendly systems. Furthermore, the principles that are formed by the
EU Data Protection Directive, which are themselves based on the Fair
Information Practices (Gellman 2014) from the early
70s—transparency, purpose, proportionality, access,
transfer—are technologically neutral and as such can also be
considered as high level ‘design principles’. Systems that
are designed with these rules and guidelines in mind should
thus—in principle—be in compliance with EU privacy laws
and respect the privacy of its users.</p>

<p>The rules and principles described above give high-level guidance
for designing privacy-preserving systems, but this does not mean that
if these methodologies are followed the resulting IT system will
(automatically) be privacy friendly. Some design principles are rather
vague and abstract. What does it mean to make a transparent design or
to design for proportionality? The principles need to be interpreted
and placed in a context when designing a specific system. But
different people will interpret the principles differently, which will
lead to different design choices, some of which will be clearly better
than others. There is also a difference between the design and the
implementation of a computer system. During the implementation phase
software bugs are introduced, some of which can be exploited to break
the system and extract private information. How to implement bug-free
computer systems remains an open research question (Hoare 2003). In
addition, implementation is another phase wherein choices and
interpretations are made: system designs can be implemented in
infinitely many ways. Moreover, it is very hard to verify—for
anything beyond non-trivial systems—whether an implementation
meets its design/specification (Loeckx, Sieber, &amp; Stansifer
1985). This is even more difficult for non-functional requirements
such as ‘being privacy preserving’ or security properties
in general.  </p>

<p>Some specific solutions to privacy problems aim at increasing the
level of awareness and consent of the user. These solutions can be
seen as an attempt to apply the notion of informed consent to privacy
issues with technology (Pieters 2011). For example, the Privacy Coach
supports customers in making privacy decisions when confronted with
RFID tags (Broenink et al. 2010). However, users have only a limited
capability of dealing with such choices, and providing too many
choices may easily lead to the problem of moral overload (van den
Hoven, Lokhorst, &amp; Van de Poel 2012). A technical solution is
support for automatic matching of a privacy policy set by the user
against policies issued by web sites or apps.</p>

<h3><a id="PriEnhTec">3.2 Privacy enhancing technologies</a></h3>

<p>A growing number of software tools are available that provide some
form of privacy (usually anonymity) for their users, such tools are
commonly known as privacy enhancing technologies (Danezis &amp;
Gürses 2010, Other Internet Resources). Examples include
communication-anonymizing tools such as Tor (Dingledine, Mathewson,
&amp; Syverson 2004) and Freenet (Clarke et al. 2001), and
identity-management systems for which many commercial software
packages exist (see below). Communication anonymizing tools allow
users to anonymously browse the web (with Tor) or anonymously share
content (Freenet). They employ a number of cryptographic techniques
and security protocols in order to ensure their goal of anonymous
communication. Both systems use the property that numerous users use
the system at the same time which provides <i>k</i>-anonymity (Sweeney
2002): no individual can be uniquely distinguished from a group of
size <i>k</i>, for large values for <i>k</i>. Depending on the system,
the value of <i>k</i> can vary between a few hundred to hundreds of
thousands. In Tor, messages are encrypted and routed along numerous
different computers, thereby obscuring the original sender of the
message (and thus providing anonymity). Similarly, in Freenet content
is stored in encrypted form from all users of the system. Since users
themselves do not have the necessary decryption keys, they do not know
what kind of content is stored, by the system, on their own
computer. This provides plausible deniability and privacy. The system
can at any time retrieve the encrypted content and send it to
different Freenet users.</p>

<p>Privacy enhancing technologies also have their downsides. For
example, Tor, the tool that allows anonymized communication and
browsing over the Internet, is susceptible to an attack whereby, under
certain circumstances, the anonymity of the user is no longer
guaranteed (Back, Möller, &amp; Stiglic 2001; Evans, Dingledine,
&amp; Grothoff 2009). Freenet (and other tools) have similar problems
(Douceur 2002). Note that for such attacks to work, an attacker needs
to have access to large resources that in practice are only realistic
for intelligence agencies of countries. However, there are other
risks.  Configuring such software tools correctly is difficult for the
average user, and when the tools are not correctly configured
anonymity of the user is no longer guaranteed. And there is always the
risk that the computer on which the privacy-preserving software runs
is infected by a Trojan horse (or other digital pest) that monitors
all communication and knows the identity of the user.</p>

<p>Another option for providing anonymity is the anonymization of data
through special software. Tools exist that remove patient names and
reduce age information to intervals: the age 35 is then represented as
falling in the range 30–40. The idea behind such anonymization
software is that a record can no longer be linked to an individual,
while the relevant parts of the data can still be used for scientific
or other purposes. The problem here is that it is very hard to
anonymize data in such a way that all links with an individual are
removed and the resulting anonymized data is still useful for research
purposes.  Researchers have shown that it is almost always possible to
reconstruct links with individuals by using sophisticated statistical
methods (Danezis, Diaz, &amp; Troncoso 2007) and by combining multiple
databases (Anderson 2008) that contain personal
information. Techniques such as <i>k</i>-anonymity might also help to
generalize the data enough to make it unfeasible to de-anonymize data
(LeFevre et al. 2005).</p>

<h3><a id="Cry">3.3 Cryptography</a></h3>

<p>Cryptography has long been used as a means to protect data, dating
back to the Caesar cipher more than two thousand years ago. Modern
cryptographic techniques are essential in any IT system that needs to
store (and thus protect) personal data. Note however that by itself
cryptography does not provide any protection against data breaching;
only when applied correctly in a specific context does it become a
‘fence’ around personal data. Cryptography is a large
field, so any description here will be incomplete. We'll focus instead
on some newer cryptographic techniques, in particular homomorphic
encryption, that have the potential to become very important for
processing and searching in personal data.</p>

<p>Various techniques exist for searching through encrypted data (Song
et al. 2000), which provides a form of privacy protection (the data is
encrypted) and selective access to sensitive data. One relatively new
technique that can be used for designing privacy-preserving systems is
‘homomorphic encryption’ (Gentry 2009). Homomorphic
encryption allows a data processor to process encrypted data, i.e.,
users could send personal data in encrypted form and get back some
useful results—for example, recommendations of movies that
online friends like—in encrypted form. The original user can
then again decrypt the result and use it without revealing any
personal data to the data processor.  Homomorphic encryption, for
example, could be used to aggregate encrypted data thereby allowing
both privacy protection and useful (anonymized) aggregate
information. The technique is currently still in its infancy; it does
not scale yet to the large amounts of data stored in today's
systems. However, if homomorphic encryption could be made to work more
efficiently the results have the potential to be revolutionary, at
least for privacy-preserving systems.</p>

<h3><a id="IdeMan">3.4 Identity management</a></h3>

<p>The use and management of user's online identifiers are crucial in
the current Internet and social networks. Online reputations become
more and more important, both for users and for companies. In the era
of ‘Big Data’ correct information about users has an
increasing monetary value.</p>

<p>‘Single sign on’ frameworks, provided by independent
third parties (OpenID) but also by large companies such as Facebook,
Microsoft and Google (Ko et al. 2010), make it easy for users to
connect to numerous online services using a single online
identity. These online identities are usually directly linked to the
real world (off line) identities of individuals; indeed Facebook,
Google and others require this form of log on (den Haak
2012). Requiring a direct link between online and ‘real
world’ identities is problematic from a privacy perspective,
because they allow profiling of users (Benevenuto et al. 2012). Not
all users will realize how large the amount of data is that companies
gather in this manner, or how easy it is to build a detailed profile
of users. Profiling becomes even easier if the profile information is
combined with other techniques such as implicit authentication via
cookies and tracking cookies (Mayer &amp; Mitchell 2012).</p>

<p>From a privacy perspective a better solution would be the use of
attribute-based authentication (Goyal et al. 2006) which allows access
of online services based on the attributes of users, for example their
friends, nationality, age etc. Depending on the attributes used, they
might still be traced back to specific individuals, but this is no
longer crucial. In addition, users can no longer be tracked to
different services because they can use different attributes to access
different services which makes it difficult to trace online identities
over multiple transactions, thus providing unlinkability for the
user.</p>

<h2><a id="EmeTecOurUndPri">4. Emerging technologies and our understanding of privacy</a></h2>

<p>In the previous sections, we have outlined how current technologies
may impact privacy, as well as how they may contribute to mitigating
undesirable effects. However, there are future and emerging
technologies that may have an even more profound impact. Consider for
example brain-computer interfaces. In case computers are connected
directly to the brain, not only behavioral characteristics are subject
to privacy considerations, but even one's thoughts run the risk of
becoming public, with decisions of others being based upon them. In
addition, it could become possible to change one's behavior by means
of such technology. Such developments therefore require further
consideration of the reasons for protecting privacy. In particular,
when brain processes could be influenced from the outside, autonomy
would be a value to reconsider to ensure adequate protection.</p>

<p>Apart from evaluating information technology against current moral
norms, one also needs to consider the possibility that technological
changes influence the norms themselves (Boenink, Swierstra &amp;
Stemerding 2010). Technology thus does not only influence privacy by
changing the accessibility of information, but also by changing the
privacy norms themselves. For example, social networking sites invite
users to share more information than they otherwise might. This
“oversharing” becomes accepted practice within certain
groups. With future and emerging technologies, such influences can
also be expected and therefore they ought to be taken into account
when trying to mitigate effects.</p>

<p>Another fundamental question is whether, given the future (and even
current) level of informational connectivity, it is feasible to
protect privacy by trying to hide information from parties who may use
it in undesirable ways. Gutwirth &amp; De Hert (2008) argue that it
may be more feasible to protect privacy by transparency—by
requiring actors to justify decisions made about individuals, thus
insisting that decisions are not based on illegitimate
information. This approach comes with its own problems, as it might be
hard to prove that the wrong information was used for a
decision. Still, it may well happen that citizens, in turn, start data
collection on those who collect data about them, e.g.,
governments. Such “counterveillance” or sousveillance may
be used to gather information about the use of information, thereby
improving accountability. The open source movement may also contribute
to transparency of data processing. In this context, transparency can
be seen as a pro-ethical condition contributing to privacy (Turilli
&amp; Floridi 2009).</p>

<p>It has been argued that the precautionary principle, well known in
environmental ethics, might have a role in dealing with emerging
information technologies as well (Pieters &amp; van Cleeff 2009; Som,
Hilty &amp; Köhler 2009). The principle would see to it that the
burden of proof for absence of irreversible effects of information
technology on society, e.g., in terms of power relations and equality,
would lie with those advocating the new technology. Precaution, in
this sense, could then be used to impose restrictions at a regulatory
level, in combination with or as an alternative to empowering users,
thereby potentially contributing to the prevention of moral or
informational overload on the user side. Apart from general debates
about the desirable and undesirable features of the precautionary
principle, challenges to it lie in its translation to social effects
and social sustainability, as well as to its application to
consequences induced by intentional actions of agents. Whereas the
occurrence of natural threats or accidents is probabilistic in nature,
those who are interested in improper use of information behave
strategically, requiring a different approach to risk (i.e., security
as opposed to safety). In addition, proponents of precaution will need
to balance it with other important principles, viz., of informed
consent and autonomy.</p>

<p>Finally, it is appropriate to note that not all social effects of
information technology concern privacy. Examples include the effects
of social network sites on friendship, and the verifiability of
results of electronic elections. Therefore, value-sensitive design
approaches and impact assessments of information technology should not
focus on privacy only, since information technology affects many other
values as well.</p>

</div>

<div id="bibliography">

<h2><a id="Bib">Bibliography</a></h2>

<ul class="hanging">

<li>Allen, A., 2011, <em>Unpopular Privacy: What Must We Hide?</em>
Oxford: Oxford University Press.</li>

<li>Anderson, R.J., 2008, <em>Security Engineering: A guide to
building dependable distributed systems</em>, Indianapolis, IN: Wiley.</li>

<li>Back, A., U. Möller, &amp; A. Stiglic, 2001, “Traffic
analysis attacks and trade-offs in anonymity providing systems”,
in <em>Information Hiding</em>, Berlin: Springer, pp. 245–257.</li>

<li>Bellman, S., E.J. Johnson, &amp; G.L. Lohse, 2001, “On
site: to opt-in or opt-out?: it depends on the question”,
<em>Communications of the ACM</em>, 44(2): 25–27.</li>

<li>Benevenuto, F., T. Rodrigues, M. Cha, &amp; V. Almeida, 2012,
“Characterizing user navigation and interactions in online
social networks”, <em>Information Sciences</em>, 195:
1–24.</li>

<li>Blaauw. M.J., 2013, “The Epistemic Account of
Privacy”, <em>Episteme</em>, 10(2): 167–177.</li>

<li>Boenink, M., T. Swierstra, &amp; D. Stemerding, 2010,
“Anticipating the interaction between technology and morality: a
scenario study of experimenting with humans in
bionanotechnology”, <em>Studies in Ethics, Law, and
Technology</em>, 4(2): 1–38.  doi:10.2202/1941-6008.1098</li>

<li>Brey, P., 2005, “Freedom and privacy in ambient
intelligence”, <em>Ethics and Information Technology</em>, 7(3):
157–166.</li>

<li>Broenink, G., J.H. Hoepman, C.V.T.  Hof, R.  Van Kranenburg,
D. Smits, &amp; T.  Wisman, 2010, “The privacy coach: Supporting
customer privacy in the internet of things”, 
<em>arXiv preprint</em> 1001.4459 
 [<a href="http://arxiv.org/abs/1001.4459" target="other">available online</a>].
</li>

<li>Cavoukian, A., 2009, <em>Privacy by Design</em>, Ottowa:
Information and Privacy Commissioner of Ontario, Canada.
 [<a href="https://web.archive.org/web/20160330122525/https://www.ipc.on.ca/images/resources/privacybydesign.pdf" target="other">Cavoukian 2009 available online</a> (PDF)].</li>

<li>–––, 2010, “Privacy by Design: The
Definitive workshop”, <em>Identity in the Information
Society</em>, 3(2): 121–126.</li>

<li>Clarke, R., 2009, “Privacy impact assessment: Its origins
and development”, <em>Computer law &amp; security review</em>,
25(2): 123–135.</li>

<li>Clarke, I., O. Sandberg, B. Wiley, &amp; T. Hong, 2001,
“Freenet: A distributed anonymous information storage and
retrieval system”, in <em>Designing Privacy Enhancing
Technologies</em>, Berlin: Springer, pp. 46–66.</li>

<li>Dahl, J. Y., &amp; A.R. Sætnan, 2009, “It all
happened so slowly: On controlling function creep in forensic DNA
databases”, <em>International journal of law, crime and
justice</em>, 37(3): 83–103.</li>

<li>Danezis, G., C. Diaz, &amp; C. Troncoso, 2007, “Two-sided
statistical disclosure attack”, in <em>Proceedings of the 7th
international conference on Privacy enhancing technologies</em>,
Berlin: Springer, pp. 30–44.</li>

<li>DeCew, Judith Wagner, 1997, <em>Pursuit of Privacy: Law, Ethics,
and the Rise of Technology</em>, Ithaca, NY: Cornell University
Press.</li>

<li>Dechesne, F., M. Warnier, &amp; J. van den Hoven, 2013,
“Ethical requirements for reconfigurable sensor technology: a
challenge for value sensitive design”, <em>Ethics and
Information Technology</em>, 15(3): 173–181.</li>

<li>Delaune, S., S. Kremer, &amp; M. Ryan, 2006,
“Coercion-resistance and receipt-freeness in electronic
voting”, in the <em>Proceedings of the 19th IEEE Computer
Security Foundations Workshop</em>, IEEE Computer Society Press, pages
28–39.
 [<a href="http://www.loria.fr/~skremer/Publications/b2hd-DKR-csfw06.html" target="other">Delaune et al. 2006 available online</a>]</li>

<li>Dingledine, R., N. Mathewson, &amp; P. Syverson, 2004,
“Tor: The second-generation onion router”, in
<em>Proceedings of the 13th conference on USENIX Security
Symposium</em> (Volume 13), Berkeley, CA: USENIX Association,
 pp. 303–320
 [<a href="https://www.usenix.org/legacy/publications/library/proceedings/sec04/tech/full_papers/dingledine/dingledine.pdf?CFID=597263069&amp;CFTOKEN=54138242" target="other">Dingledine et al. 2004 available online (pdf)</a>]
</li>

<li>Douceur, J., 2002, “The Sybil attack”, in
<em>Peer-to-peer Systems</em>, Berlin: Springer, pp. 251–260.</li>

<li>Ellison, N. B., 2007, “Social network sites: Definition,
history, and scholarship”, <em>Journal of Computer-Mediated
Communication</em>, 13(1): 210–230.</li>

<li>Evans, N.S., R. Dingledine, &amp; C. Grothoff, 2009, “A
practical congestion attack on Tor using long paths”, in
<em>Proceedings of the 18th conference on USENIX security
symposium</em>, Berkeley, CA: USENIX Association, pp. 33–50. 
 [<a href="https://www.usenix.org/legacy/events/sec09/tech/full_papers/evans.pdf" target="other">Evans et al. 2009 available online</a>]</li>


<li>Floridi, L., 2008, “Artificial intelligence's new frontier:
Artificial companions and the fourth revolution”,
<em>Metaphilosophy</em>, 39(4–5): 651–655.</li>

<li>Friedewald, M. &amp; O. Raabe, 2011, “Ubiquitous computing:
An overview of technology impacts”, <em>Telematics and
Informatics</em>, 28(2): 55–65.</li>

<li>Friedman, B., P.H. Kahn, Jr, &amp; A. Borning, 2006, “Value
sensitive design and information systems”, in <em>Human-computer
interaction in management information systems: Foundations</em>,
P. Zhang &amp; D. Galletta (eds.), Armonk: M.E. Sharp, 4.</li>

<li>Gentry, C., 2009, “Fully homomorphic encryption using ideal
lattices”, in <em>Proceedings of the 41st annual ACM symposium
on Theory of computing</em>, ACM, pp. 169–178. </li>

<li>Goyal, V., O. Pandey, A. Sahai, &amp; B. Waters, 2006,
“Attribute-based encryption for fine-grained access control of
encrypted data”, in <em>Proceedings of the 13th ACM conference
on Computer and communications security</em>, ACM,
pp. 89–98.</li>

<li>Gutwirth, S. &amp; P. De Hert, 2008, “Regulating profiling
in a democratic constitutional state”, in Hildebrandt and
Gutwirth 2008: 271–302.</li>

<li>den Haak, B., 2012, “Integrating user customization and
authentication: the identity crisis”, <em>Security &amp;
Privacy, IEEE</em>, 10(5): 82–85.</li>

<li>Heersmink, R., J. van den Hoven, N.J. van Eck, &amp; J. van den
Berg, 2011. “Bibliometric mapping of computer and information
ethics”, <em>Ethics and information technology</em>, 13(3):
241–249.</li>

<li>Hildebrandt, M., 2008, “Defining Profiling: A New Type of
Knowledge?” in Hildebrandt and Gutwirth 2008: 17–45.</li>

<li>Hildebrandt, M. &amp; S. Gutwirth (eds.), 2008, <em>Profiling the
European Citizen: Cross-disciplinary Perspectives</em>, Dordrecht:
Springer Netherlands.</li>

<li>Hoare, T., 2003, “The verifying compiler: A grand challenge
for computing research”, in <em>Proceedings of the 12th
international conference on Compiler construction</em>, Berlin:
Springer, pp. 262–272.</li>

<li>Hone, K. &amp; J.H.P. Eloff, 2002, “Information security
policy—what do international information security standards
say?”, <em>Computers &amp; Security</em>, 21(5):
402–409.</li>

<li>van den Hoven, J., 1999, “Privacy and the Varieties of
Informational Wrongdoing”, <em>Australian Journal of
Professional and Applied Ethics</em>, 1(1): 30–44.</li>

<li>–––, 2008, “Information technology,
privacy, and the protection of personal data”, in
<em>Information technology and moral philosophy</em>, J. Van Den Hoven
and J. Weckert (eds.), Cambridge: Cambridge University Press,
pp. 301–322.</li>

<li>van den Hoven, J., G.J. Lokhorst, &amp; I. Van de Poel, 2012,
“Engineering and the problem of moral overload”,
<em>Science and engineering ethics</em>, 18(1): 143–155.</li>

<li>Ko, M.N., G.P. Cheek, M. Shehab, &amp; R. Sandhu, 2010,
“Social-networks connect services”, <em>Computer</em>,
43(8): 37–43.</li>

<li>Krishnamurthy, B. &amp; C.E. Wills, 2009. “On the leakage
of personally identifiable information via online social
networks”, in <em>Proceedings of the 2nd ACM workshop on Online
social networks</em>, ACM, pp. 7–12.</li>

<li>LeFevre, K., D.J. DeWitt, &amp; R. Ramakrishnan, 2005,
“Incognito: Efficient full-domain k-anonymity”, in
<em>Proceedings of the 2005 ACM SIGMOD international conference on
Management of data</em>, ACM, pp. 49–60.</li>

<li>Loeckx, J., K. Sieber, &amp; R.D. Stansifer, 1985, <em>The
foundations of program verification</em>, Chichester: John Wiley &amp; Sons.</li>

<li>Matheson, David, 2007, “Unknowableness and Informational
Privacy”, <em>Journal of Philosophical Research</em>, 32:
251–67.</li>

<li>Mayer, J.R. &amp; J.C. Mitchell, 2012, “Third-party web
tracking: Policy and technology”, in <em>Security and Privacy
(SP) 2012 IEEE Symposium on</em>, IEEE, pp. 413–427.</li>

<li>Michener, J., 1999, “System insecurity in the Internet
age”, <em>Software</em>, IEEE, 16(4): 62–69.</li>

<li>Nissenbaum, Helen, 2004, “Privacy as Contextual
Integrity”, <em>Washington Law Review</em>, 79:
101–139.</li>

<li>OECD, 1980, <em>Guidelines on the Protection of Privacy and
Transborder Flows of Personal Data</em>, OECD. 
 [<a href="http://www.oecd.org/internet/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.htm" target="other">OECD 1980 available online</a>]</li>

<li>Palmer, D.E., 2005, “Pop-ups, cookies, and spam: toward a
deeper analysis of the ethical significance of internet marketing
practices”, <em>Journal of business ethics</em>, 58(1–3):
271–280.</li>

<li>Pieters, W., 2011, “Explanation and trust: what to tell the
user in security and AI?”, <em>Ethics and information
technology</em>, 13(1): 53–64.</li>

<li>–––, 2013, “On thinging things and
serving services: technological mediation and inseparable
goods”, <em>Ethics and information technology</em>, 15(3):
195–208.</li>

<li>Pieters, W. &amp; A. van Cleeff, 2009, “The precautionary
principle in a world of digital dependencies”,
<em>Computer</em>, 42(6): 50–56.</li>


<li>Rössler, Beate (ed.), 2004, <em>Privacies: Philosophical
Evaluations</em>, Stanford, CA: Stanford University Press.</li>

<li>Rubel, Alan, 2011, “The Particularized Judgment Account of
Privacy”, <em>Res Publica</em>, 17(3): 275–90.</li>

<li>Ruiter, J. &amp; M. Warnier, 2011, “Privacy Regulations for
Cloud Computing: Compliance and Implementation in Theory and
Practice”, in <em>Computers, Privacy and Data Protection: an
Element of Choice</em>, S. Gutwirth, Y. Poullet, P. De Hert, and
R. Leenes (eds.), Dordrecht: Springer
Netherlands, pp. 361–376.</li>

<li>Solove, D., 2006, “A Taxonomy of
Privacy”, <em>University of Pennsylvania Law Review</em>, 154:
477–564.</li>

<li>Som, C., L.M. Hilty, &amp; A.R. Köhler, 2009, “The
precautionary principle as a framework for a sustainable information
society”, <em>Journal of business ethics</em>, 85(3):
493–505.</li>

<li>Song, D.X., D. Wagner, &amp; A. Perrig, 2000, “Practical
techniques for searches on encrypted data”, in <em>Security and
Privacy, 2000. S&amp;P 2000. Proceedings. 2000 IEEE Symposium on</em>,
IEEE, pp. 44–55.</li>

<li>Sweeney, L., 2002, “K-anonymity: A model for protecting
privacy”, <em>International Journal of Uncertainty, Fuzziness
and Knowledge-Based Systems</em>, 10(05): 557–570.</li>

<li>Tavani, H.T., 2004, “Genomic research and data-mining
technology: Implications for personal privacy and informed
consent”, <em>Ethics and information technology</em>, 6(1):
15–28.</li>

<li>Thomson, Judith Jarvis, 1975, “The Right to Privacy”,
<em>Philosophy and Public Affairs</em>, 4: 295–314.</li>

<li>Turilli, M. &amp; L. Floridi, 2009, “The ethics of
information transparency”, <em>Ethics and Information
Technology</em>, 11(2): 105–112.</li>

<li>Warren, Samuel D. &amp; Louis D. Brandeis, 1890, “The Right
to Privacy”, <em>Harvard Law Review</em>, 4(5):
193–220. [<a href="http://www.jstor.org/stable/1321160" target="other">Warren and Brandeis 1890 available online</a>]</li>

<li>Westin, Alan F., 1967, <em>Privacy and Freedom</em>, New York:
Atheneum.</li>

</ul>

</div>

<div id="academic-tools">

<h2><a id="Aca">Academic Tools</a></h2>

<blockquote>
<table>
<tbody><tr>
<td valign="top"><img src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/sepman-icon.jpg" alt="sep man icon"></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=it-privacy" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td valign="top"><img src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/sepman-icon.jpg" alt="sep man icon"></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/it-privacy/" target="other">Preview the PDF version of this entry</a> at the
<a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td valign="top"><img src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/inpho.png" alt="inpho icon"></td>
<td><a href="https://www.inphoproject.org/entity?sep=it-privacy&amp;redirect=True" target="other">Look up this entry topic</a>
at the <a href="https://www.inphoproject.org/" target="other">Indiana Philosophy Ontology Project</a>
(InPhO).</td>
</tr>

<tr>
<td valign="top"><img src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/pp.gif" alt="phil papers icon"></td>
<td><a href="http://philpapers.org/sep/it-privacy/" target="other">Enhanced bibliography for this entry</a>
at <a href="http://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</tbody></table>
</blockquote>

</div>

<div id="other-internet-resources">

<h2><a id="Oth">Other Internet Resources</a></h2>

<ul>

<li>Danezis, G &amp; S. Gürses, 2010, 
“<a href="http://homes.esat.kuleuven.be/~sguerses/papers/DanezisGuersesSurveillancePets2010.pdf" target="other">A critical review of 10 years of Privacy Technology</a>.”</li>

<li>Gellman, R., 2014, 
 “<a href="http://bobgellman.com/rg-docs/rg-FIPShistory.pdf" target="other">Fair information practices: a basic history</a>,”, 
  Version 2.12, August 3, 2014, online manuscript.</li>

<li>PCI DSS (= Payment Card Industry Data Security Standard), v3.0 (2013),
<a href="http://www.pcisecuritystandards.org/documents/PCI_DSS_v3.pdf" target="other">Requirements and Security Assessment Procedures</a>,
  PCI Security Standards Council, LLC.</li>

<li>Richter, H., W. Mostowski, &amp; E. Poll, 2008,
“<a href="http://cs.ru.nl/E.Poll/papers/nluug.pdf" target="other">Fingerprinting passports</a>”,
 presented at NLUUG Spring Conference on Security.</li>

<li><a href="http://epic.org/" target="other">Electronic Privacy Information Center </a>.</li>

<li><a href="http://ec.europa.eu/justice/data-protection/" target="other">European Commission, Protection of personal data</a></li>

<li><a href="http://www.state.gov/m/a/ips/c36513.htm" target="other">US Department of State, Privacy Act</a>.</li>
</ul>

</div>

<div id="related-entries">

<h2><a id="Rel">Related Entries</a></h2>

<p>

 <a href="https://plato.stanford.edu/entries/ethics-computer/">computer and information ethics</a> |
 <a href="https://plato.stanford.edu/entries/computing-responsibility/">computing: and moral responsibility</a> |
 <a href="https://plato.stanford.edu/entries/ethics-search/">ethics: search engines and</a> |
 <a href="https://plato.stanford.edu/entries/information/">information</a> |
 <a href="https://plato.stanford.edu/entries/it-moral-values/">information technology: and moral values</a> |
 <a href="https://plato.stanford.edu/entries/privacy/">privacy</a> |
 <a href="https://plato.stanford.edu/entries/ethics-social-networking/">social networking and ethics</a>

</p>

</div>

</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="https://plato.stanford.edu/info.html#c">Copyright © 2014</a> by

<br>
Jeroen van den Hoven
&lt;<a href="mailto:m%2ej%2evandenhoven%40tudelft%2enl"><em>m<abbr title=" dot ">.</abbr>j<abbr title=" dot ">.</abbr>vandenhoven<abbr title=" at ">@</abbr>tudelft<abbr title=" dot ">.</abbr>nl</em></a>&gt;<br>
Martijn Blaauw
&lt;<a href="mailto:M%2eJ%2eBlaauw%40tudelft%2enl"><em>M<abbr title=" dot ">.</abbr>J<abbr title=" dot ">.</abbr>Blaauw<abbr title=" at ">@</abbr>tudelft<abbr title=" dot ">.</abbr>nl</em></a>&gt;<br>
<a href="http://homepage.tudelft.nl/e7x9k/" target="other">Wolter Pieters</a>
&lt;<a href="mailto:W%2ePieters%40tudelft%2enl"><em>W<abbr title=" dot ">.</abbr>Pieters<abbr title=" at ">@</abbr>tudelft<abbr title=" dot ">.</abbr>nl</em></a>&gt;<br>
<a href="http://homepage.tudelft.nl/68x7e/" target="other">Martijn Warnier</a>
&lt;<a href="mailto:M%2eE%2eWarnier%40tudelft%2enl"><em>M<abbr title=" dot ">.</abbr>E<abbr title=" dot ">.</abbr>Warnier<abbr title=" at ">@</abbr>tudelft<abbr title=" dot ">.</abbr>nl</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
   <div id="article-banner-content">
  <a href="https://plato.stanford.edu/fundraising/">
  Open access to the SEP is made possible by a world-wide funding initiative.<br>
  Please Read How You Can Help Keep the Encyclopedia Free</a>
 </div>

</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li><a href="https://plato.stanford.edu/contents.html">Table of Contents</a></li>
            <li><a href="https://plato.stanford.edu/new.html">What's New</a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li><a href="https://plato.stanford.edu/published.html">Chronological</a></li>
            <li><a href="https://plato.stanford.edu/archives/">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li><a href="https://plato.stanford.edu/info.html">Editorial Information</a></li>
            <li><a href="https://plato.stanford.edu/about.html">About the SEP</a></li>
            <li><a href="https://plato.stanford.edu/board.html">Editorial Board</a></li>
            <li><a href="https://plato.stanford.edu/cite.html">How to Cite the SEP</a></li>
            <li><a href="https://plato.stanford.edu/special-characters.html">Special Characters</a></li>
            <li><a href="https://plato.stanford.edu/tools/">Advanced Tools</a></li>
            <li><a href="https://plato.stanford.edu/contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li><a href="https://plato.stanford.edu/support/">Support the SEP</a></li>
            <li><a href="https://plato.stanford.edu/support/friends.html">PDFs for SEP Friends</a></li>
            <li><a href="https://plato.stanford.edu/support/donate.html">Make a Donation</a></li>
            <li><a href="https://plato.stanford.edu/support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
                <div class="btn-group">
<a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/"><span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span><span class="mirror-source">CSLI, Stanford University</span></a>          <ul class="dropdown-menu">
            <li><a href="https://stanford.library.sydney.edu.au/entries/it-privacy/"><span class="flag flag-australia"></span> Australia <span class="mirror-source">Library, University of Sydney</span></a>           </li>
            <li><a href="https://seop.illc.uva.nl/entries/it-privacy/"><span class="flag flag-netherlands"></span> Netherlands <span class="mirror-source">ILLC, University of Amsterdam</span></a>           </li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p class="csli-logo"><a href="https://www-csli.stanford.edu/"><img src="./Privacy and Information Technology (Stanford Encyclopedia of Philosophy)_files/SU_csli.png" width="355" alt="Stanford Center for the Study of Language and Information"></a></p>
        <p>The Stanford Encyclopedia of Philosophy is <a href="https://plato.stanford.edu/info.html#c">copyright © 2016</a> by <a href="http://mally.stanford.edu/">The Metaphysics Research Lab</a>, Center for the Study of Language and Information (CSLI), Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40353515-1', 'stanford.edu');
  ga('send', 'pageview');

</script>




</body></html>